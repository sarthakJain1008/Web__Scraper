#!/bin/bash
# start-optimized-scraper.sh
# Launch optimized scraper with 10 APIs - divides only remaining listings

echo "ğŸš€ Starting OPTIMIZED Parallel Detail Scraper"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "Strategy: Divide only REMAINING unprocessed listings among 10 APIs"
echo "Each API gets equal share of unprocessed work"
echo "No duplicate work, optimal efficiency"
echo ""
echo "Logs will be saved to:"
echo "  - logs/optimized-api-0.log through logs/optimized-api-9.log"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Create logs directory
mkdir -p logs

# Start all 10 workers in background
for i in {0..9}; do
  echo "ğŸš€ Starting Optimized API Worker $((i+1))/10..."
  API_KEY_INDEX=$i nohup node scrape-parallel-optimized.js > logs/optimized-api-$i.log 2>&1 &
  PID=$!
  echo "   âœ… Started with PID: $PID"
  sleep 2 # Small delay to avoid overwhelming the system
done

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… All 10 optimized workers started!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“Š Monitor progress with:"
echo "   node check-parallel-progress.js"
echo ""
echo "ğŸ“Š View logs:"
echo "   tail -f logs/optimized-api-0.log"
echo "   tail -f logs/optimized-api-1.log"
echo "   ... etc"
echo ""
echo "ğŸ“Š View all workers:"
echo "   ps aux | grep scrape-parallel-optimized"
echo ""
echo "ğŸ›‘ Stop all workers:"
echo "   pkill -f scrape-parallel-optimized"
echo ""
